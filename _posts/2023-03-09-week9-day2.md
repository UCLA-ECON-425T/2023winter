---
layout: post_teaching
title: Lecture 18
category: econ425twinter2023
---

## Today

* Neural network: practice, Keras Tuner workflow.

## FAQs

* Ensemble methods: [bagging](https://ucla-econ-425t.github.io/2023winter/slides/08-tree/tree.html#bagging) and [boosting](https://ucla-econ-425t.github.io/2023winter/slides/08-tree/tree.html#boosting). Not tied to tree methods. Can use any (weak) learner: the `estimator` argument in the [sklearn.ensemble.BaggingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) and [sklearn.ensemble.AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier) functions. 

* For the [random forest](https://ucla-econ-425t.github.io/2023winter/slides/08-tree/tree.html#random-forests) decorrelation trick, is the random subset of predictors fixed for a tree or changing at each split? 

* AdaBoost vs Gradient Boosting vs XGBoost. Gradient boosting is what we discussed in class [link](https://ucla-econ-425t.github.io/2023winter/slides/08-tree/tree.html#boosting). AdaBoost is doing exponential weighting of each observation at each boosting iteration (see, e.g., ESL Algorithm 10.1, p339). 

* HW5 (SVM) bonus question. 

* Why does the [gradient descent (GD) algorithm](https://ucla-econ-425t.github.io/2023winter/slides/10-nn/nn.html#fig-gd) move towards a local (minimum)? Variants of GD: [Keras optimizers](https://keras.io/api/optimizers/#available-optimizers). Today's lecture.

* [Loss function for GAN](https://ucla-econ-425t.github.io/2023winter/slides/10-nn/nn.html#generative-adversarial-networks-gans). 

## Announcement

* [HW6](https://ucla-econ-425t.github.io/2023winter/hw/hw6/hw6.html) is due Mar 24 @ 11:59pm. Start early; use Slack and office hours for questions. 

* UCLA Synthetic Data Symposium: <https://ucla-synthetic-data.github.io/>. Apr 13-14, 2023.

* UCLA OARC Deep Learning Workshop Series (using PyTorch): <https://github.com/huqy/deep_learning_workshops>.
