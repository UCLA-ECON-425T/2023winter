<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dr.&nbsp;Hua Zhou @ UCLA">
<meta name="dcterms.date" content="2023-02-14">

<title>Support Vector Machines (ISL 9)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="svm_files/libs/clipboard/clipboard.min.js"></script>
<script src="svm_files/libs/quarto-html/quarto.js"></script>
<script src="svm_files/libs/quarto-html/popper.min.js"></script>
<script src="svm_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="svm_files/libs/quarto-html/anchor.min.js"></script>
<link href="svm_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="svm_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="svm_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="svm_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="svm_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="toc-section-number">1</span>  Overview</a></li>
  <li><a href="#hyperplane" id="toc-hyperplane" class="nav-link" data-scroll-target="#hyperplane"><span class="toc-section-number">2</span>  Hyperplane</a></li>
  <li><a href="#maximal-margin-classifier" id="toc-maximal-margin-classifier" class="nav-link" data-scroll-target="#maximal-margin-classifier"><span class="toc-section-number">3</span>  Maximal margin classifier</a></li>
  <li><a href="#support-vector-classifier" id="toc-support-vector-classifier" class="nav-link" data-scroll-target="#support-vector-classifier"><span class="toc-section-number">4</span>  Support vector classifier</a></li>
  <li><a href="#nonlinearity-support-vector-machines-svm" id="toc-nonlinearity-support-vector-machines-svm" class="nav-link" data-scroll-target="#nonlinearity-support-vector-machines-svm"><span class="toc-section-number">5</span>  Nonlinearity: support vector machines (SVM)</a></li>
  <li><a href="#kernels" id="toc-kernels" class="nav-link" data-scroll-target="#kernels"><span class="toc-section-number">6</span>  Kernels</a></li>
  <li><a href="#heart-data-example-todo" id="toc-heart-data-example-todo" class="nav-link" data-scroll-target="#heart-data-example-todo"><span class="toc-section-number">7</span>  Heart data example (TODO)</a></li>
  <li><a href="#svm-for-more-than-2-classes" id="toc-svm-for-more-than-2-classes" class="nav-link" data-scroll-target="#svm-for-more-than-2-classes"><span class="toc-section-number">8</span>  SVM for more than 2 classes</a></li>
  <li><a href="#svm-vs-logistic-regression." id="toc-svm-vs-logistic-regression." class="nav-link" data-scroll-target="#svm-vs-logistic-regression."><span class="toc-section-number">9</span>  SVM vs logistic regression.</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Support Vector Machines (ISL 9)</h1>
<p class="subtitle lead">Econ 425T</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Dr.&nbsp;Hua Zhou @ UCLA </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 14, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Credit: This note heavily uses material from the books <a href="https://www.statlearning.com/"><em>An Introduction to Statistical Learning: with Applications in R</em></a> (ISL2) and <a href="https://hastie.su.domains/ElemStatLearn/"><em>Elements of Statistical Learning: Data Mining, Inference, and Prediction</em></a> (ESL2).</p>
<p>Display system information for reproducibility.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">R</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> IPython</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(IPython.sys_info())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'commit_hash': 'add5877a4',
 'commit_source': 'installation',
 'default_encoding': 'utf-8',
 'ipython_path': '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython',
 'ipython_version': '8.8.0',
 'os_name': 'posix',
 'platform': 'macOS-10.16-x86_64-i386-64bit',
 'sys_executable': '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3',
 'sys_platform': 'darwin',
 'sys_version': '3.10.9 (v3.10.9:1dd9be6584, Dec  6 2022, 14:37:36) [Clang '
                '13.0.0 (clang-1300.0.29.30)]'}</code></pre>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R version 4.2.2 (2022-10-31)
Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Big Sur ... 10.16

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.10       here_1.0.1        lattice_0.20-45   png_0.1-8        
 [5] rprojroot_2.0.3   digest_0.6.31     grid_4.2.2        jsonlite_1.8.4   
 [9] evaluate_0.20     rlang_1.0.6       cli_3.6.0         rstudioapi_0.14  
[13] Matrix_1.5-3      reticulate_1.28   rmarkdown_2.20    tools_4.2.2      
[17] htmlwidgets_1.6.1 xfun_0.37         yaml_2.3.7        fastmap_1.1.0    
[21] compiler_4.2.2    htmltools_0.5.4   knitr_1.42       </code></pre>
</div>
</div>
</div>
</div>
</div>
<section id="overview" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="overview"><span class="header-section-number">1</span> Overview</h2>
<ul>
<li>Support vector machines (SVMs) approach the two-class classification problem in a direct way:</li>
</ul>
<blockquote class="blockquote">
<p>We try and find a plane that separates the classes in feature space.</p>
</blockquote>
<ul>
<li><p>If we cannot, we get creative in two ways:</p>
<ol type="1">
<li>We soften what we mean by “separates”, and</li>
<li>We enrich and enlarge the feature space so that separation is possible.</li>
</ol></li>
</ul>
</section>
<section id="hyperplane" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="hyperplane"><span class="header-section-number">2</span> Hyperplane</h2>
<ul>
<li><p>A hyperplane in <span class="math inline">\(p\)</span> dimensions is a flat affine subspace of dimension <span class="math inline">\(p-1\)</span>.</p></li>
<li><p>In general the equation for a hyperplane has the form <span class="math display">\[
\beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p = 0.
\]</span></p></li>
<li><p>In <span class="math inline">\(p=2\)</span> dimensions, a hyperplane is a line.</p></li>
</ul>
<div id="fig-hyperplane-2d" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<img src="hyperplane_2d.png" width="400" height="400" class="figure-img">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Hyperplane in 2 dimensions.</figcaption><p></p>
</figure>
</div>
<ul>
<li><p>If <span class="math inline">\(\beta_0 = 0\)</span>, the hyperplane goes through the origin, otherwise not.</p></li>
<li><p>The vector <span class="math inline">\(\beta = (\beta_1, \ldots, \beta_p)\)</span> is called the normal vector. It points in a direction orthogonal to the surface of hyperplane.</p></li>
<li><p>If <span class="math inline">\(f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p\)</span>, then <span class="math inline">\(f(X) &gt; 0\)</span> for points on one side of the hyperplane, and <span class="math inline">\(f(X) &lt; 0\)</span> for points on the other.</p></li>
<li><p>If we code the colored points as <span class="math inline">\(y_i = +1\)</span> for blue, say and <span class="math inline">\(y_i = -1\)</span> for mauve, then if <span class="math inline">\(y_i \cdot f(X_i) &gt; 0\)</span> for all <span class="math inline">\(i\)</span>, <span class="math inline">\(f(X) = 0\)</span> defines a <strong>separating hyperplane</strong>.</p></li>
</ul>
<div id="fig-separating-hyperplane" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_9_2.pdf" width="600" height="375">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: Left: There are two classes of observations, shown in blue and in purple, each of which has measurements on two variables. Three separating hyperplanes, out of many possible, are shown in black. Right: A separating hyperplane is shown in black. The blue and purple grid indicates the decision rule made by a classifier based on this separating hyperplane: a test observation that falls in the blue portion of the grid will be assigned to the blue class, and a test observation that falls into the purple portion of the grid will be assigned to the purple class.</figcaption><p></p>
</figure>
</div>
</section>
<section id="maximal-margin-classifier" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="maximal-margin-classifier"><span class="header-section-number">3</span> Maximal margin classifier</h2>
<ul>
<li>Among all separating hyperplanes, find the one that makes the biggest gap or margin between the two classes.</li>
</ul>
<div id="fig-max-margin-classifier" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_9_3.pdf" width="400" height="450">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: The maximal margin hyperplane is shown as a solid line. The margin is the distance from the solid line to either of the dashed lines. The two blue points and the purple point that lie on the dashed lines are the <strong>support vectors</strong>, and the distance from those points to the hyperplane is indicated by arrows. The purple and blue grid indicates the decision rule made by a classifier based on this separating hyperplane.</figcaption><p></p>
</figure>
</div>
<ul>
<li><p>Constrained optimization: <span class="math display">\[\begin{eqnarray*}
\max\limits_{\beta_0,\beta_1,\ldots,\beta_p, M} &amp; &amp; \quad M \\
\text{subject to} &amp; &amp; \sum_{j=1}^p \beta_j^2 = 1 \\
&amp; &amp; y_i (\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}) \ge M \text{ for all } i.
\end{eqnarray*}\]</span> This is a convex quadratic program, which can be solved efficiently.</p>
<p>The constraints ensure that each observation is on the correct side of the hyperplane and at least a distance <span class="math inline">\(M\)</span> from the hyperplane.</p></li>
</ul>
</section>
<section id="support-vector-classifier" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="support-vector-classifier"><span class="header-section-number">4</span> Support vector classifier</h2>
<ul>
<li>Non-separable data. Sometimes the data are not separable by a linear boundary. This if often the case, unless <span class="math inline">\(N &lt; p\)</span>.</li>
</ul>
<div id="fig-noisy-data" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_9_4.pdf" width="400" height="450">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: The two classes are not separable by a hyperplane, and so the maximal margin classifier cannot be used.</figcaption><p></p>
</figure>
</div>
<ul>
<li>Noisy data. Sometimes the data are separable, but noisy. This can lead to a poor solution for the maximal-margin classifier.</li>
</ul>
<p align="center">
<embed src="ISL_fig_9_5.pdf" width="600" height="350">
</p>
<ul>
<li><p>The <strong>support vector classifier</strong> maximizes a <strong>soft</strong> margin. <span class="math display">\[\begin{eqnarray*}
\max\limits_{\beta_0,\beta_1,\ldots,\beta_p, \epsilon_1, \ldots, \epsilon_n} &amp; &amp; \quad M \\
\text{subject to} &amp; &amp; \sum_{j=1}^p \beta_j^2 = 1 \\
&amp; &amp; y_i (\beta_0 + \beta_1 x_{i1} + \cdots + \beta_p x_{ip}) \ge M (1-\epsilon_i) \\
&amp; &amp; \epsilon_i \ge 0, \sum_{i=1}^n \epsilon_i \le C.
\end{eqnarray*}\]</span> <span class="math inline">\(M\)</span> is the width of the margin.</p></li>
<li><p><span class="math inline">\(\epsilon_1, \ldots, \epsilon_n\)</span> are <strong>slack variables</strong> that allow individual observations to be on the wrong side of the margin or the hyperplane.</p>
<ul>
<li>If <span class="math inline">\(\epsilon_i = 0\)</span>, then the <span class="math inline">\(i\)</span>-th observation is on the correct side of the margin.<br>
</li>
<li>If <span class="math inline">\(0 &lt; \epsilon_i \le 1\)</span>, then the <span class="math inline">\(i\)</span>-th observation is on the wrong side of the margin.<br>
</li>
<li>If <span class="math inline">\(\epsilon_i &gt; 1\)</span>, then the <span class="math inline">\(i\)</span>-th observation is on the wrong side of the hyperplane.</li>
</ul></li>
</ul>
<div id="fig-soft-margin-classifier" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_9_6.pdf" width="600" height="350">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Left: A support vector classifier was fit to a small data set. The hyperplane is shown as a solid line and the margins are shown as dashed lines. Purple observations: Observations 3; 4; 5, and 6 are on the correct side of the margin, observation 2 is on the margin, and observation 1 is on the wrong side of the margin. Blue observations: Observations 7 and 10 are on the correct side of the margin, observation 9 is on the margin, and observation 8 is on the wrong side of the margin. No observations are on the wrong side of the hyperplane. Right: Same as left panel with two additional points, 11 and 12. These two observations are on the wrong side of the hyperplane and the wrong side of the margin.</figcaption><p></p>
</figure>
</div>
<ul>
<li><p><span class="math inline">\(C\)</span> is a regularization parameter that controls the amount that the margin can be violated by the <span class="math inline">\(n\)</span> observations.</p>
<p><span class="math inline">\(C\)</span> controls the bias-varaince trade-off.</p></li>
</ul>
<div id="fig-C" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_9_7.pdf" width="600" height="600">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6: The largest value of <span class="math inline">\(C\)</span> was used in the top left panel, and smaller values were used in the top right, bottom left, and bottom right panels. When <span class="math inline">\(C\)</span> is large, then there is a high tolerance for observations being on the wrong side of the margin, and so the margin will be large. As <span class="math inline">\(C\)</span> decreases, the tolerance for observations being on the wrong side of the margin decreases, and the margin narrows.</figcaption><p></p>
</figure>
</div>
</section>
<section id="nonlinearity-support-vector-machines-svm" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="nonlinearity-support-vector-machines-svm"><span class="header-section-number">5</span> Nonlinearity: support vector machines (SVM)</h2>
<ul>
<li>Sometime a linear boundary simply won’t work, no matter what value of <span class="math inline">\(C\)</span>.</li>
</ul>
<div id="fig-nonlinear-data" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_9_8.pdf" width="600" height="400">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7: Left: The observations fall into two classes, with a non-linear boundary between them. Right: The support vector classifier seeks a linear boundary, and consequently performs very poorly.</figcaption><p></p>
</figure>
</div>
<ul>
<li>Enlarge the space of features by including transformations: <span class="math inline">\(X_1^2, X_1^3, X_1 X_2, X_1 X_2^2, \ldots\)</span> Hence go from a <span class="math inline">\(p\)</span>-dimensional space to a <span class="math inline">\(M &gt; p\)</span> dimensional space. <span class="math display">\[
\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1^2 + \beta_4 X_2^2 + \beta_5 X_1 X_2 + \beta_6 X_1^3 + \beta_7 X_2^3 + \beta_8 X_1 X_2^2 + \beta_9 X_1^2 X_2 = 0.
\]</span>
<p align="center">
<embed src="ISL_fig_9_9.pdf" width="600" height="400">
</p></li>
</ul>
</section>
<section id="kernels" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="kernels"><span class="header-section-number">6</span> Kernels</h2>
<ul>
<li><p>Polynomials (especially high-dimensional ones) get wild rather fast.</p></li>
<li><p>There is a more elegant and controlled way to introduce nonlinearities in support-vector classifiers through the use of <strong>kernels</strong>.</p></li>
<li><p><strong>Inner product</strong> between two vectors: <span class="math display">\[
\langle x_i, x_{i'} \rangle = \sum_{j=1}^p x_{ij} x_{i'j}.
\]</span></p></li>
<li><p>The linear support vector classifier can be represented as <span class="math display">\[
f(x) = \beta_0 + \sum_{i=1}^n \alpha_i \langle x, x_i \rangle.
\]</span> To estimate the parameters <span class="math inline">\(\alpha_1,\ldots,\alpha_n\)</span> and <span class="math inline">\(\beta_0\)</span>, all we need are the <span class="math inline">\(\binom{n}{2}\)</span> inner products <span class="math inline">\(\langle x, x_i \rangle\)</span> between all pairs of training observations.</p></li>
<li><p>It turns out that most of the <span class="math inline">\(\hat \alpha_i\)</span> (non-support vectors) can be zero: <span class="math display">\[
f(x) = \beta_0 + \sum_{i \in \mathcal{S}} \hat{\alpha}_i \langle x, x_i \rangle.
\]</span> <span class="math inline">\(\mathcal{S}\)</span> is the <strong>support set</strong> of indices <span class="math inline">\(i\)</span> such that <span class="math inline">\(\hat{\alpha}_i &gt; 0\)</span>.</p></li>
<li><p>If we can compute inner products between observations, we can fit a SV classifier. Can be quite abstract!</p></li>
<li><p>Some special kernel functions can do this for us. E.g. the <strong>polynomial kernel</strong> of degree <span class="math inline">\(d\)</span> <span class="math display">\[
K(x_i, x_{i'}) = \left(1 + \sum_{j=1}^p x_{ij} x_{i'j} \right)^d
\]</span> computes the inner products needed for <span class="math inline">\(d\)</span>-dimensional polynomials.</p></li>
<li><p>The solution has the form <span class="math display">\[
f(x) = \beta_0 + \sum_{i \in \mathcal{S}} \hat{\alpha}_i K(x, x_i).
\]</span></p></li>
<li><p>Radial kernel: <span class="math display">\[
K(x_i, x_{i'}) = exp\left( - \gamma \sum_{j=1}^p (x_{ij} - x_{i'j})^2 \right).
\]</span> The solution has the form <span class="math display">\[
f(x) = \beta_0 + \sum_{i \in \mathcal{S}} \hat{\alpha}_i K(x, x_i).
\]</span> Implicit feature space is very high dimensional. Controls variance by squashing down most dimensions severely.</p></li>
</ul>
<p align="center">
<embed src="ISL_fig_9_9.pdf" width="800" height="400">
</p>
</section>
<section id="heart-data-example-todo" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="heart-data-example-todo"><span class="header-section-number">7</span> Heart data example (TODO)</h2>
</section>
<section id="svm-for-more-than-2-classes" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="svm-for-more-than-2-classes"><span class="header-section-number">8</span> SVM for more than 2 classes</h2>
<ul>
<li><p>What do we do if we have <span class="math inline">\(K &gt; 2\)</span> classes?</p></li>
<li><p><strong>OVA</strong> One versus All. Fit <span class="math inline">\(K\)</span> different 2-class SVM classifiers <span class="math inline">\(hat f_k(x)\)</span>, <span class="math inline">\(k=1,\ldots,K\)</span>; each class versus the rest. Classify <span class="math inline">\(x^*\)</span> to the class for which <span class="math inline">\(\hat f_x(x^*)\)</span> is largest.</p></li>
<li><p><strong>OVO</strong> One versus One. Fit all <span class="math inline">\(\binom{K}{2}\)</span> pairwise classifiers <span class="math inline">\(\hat f_{k\ell}(x)\)</span>. Classify <span class="math inline">\(x^*\)</span> to the class that wins the most pairwise competitions.</p></li>
<li><p>Which to choose? If <span class="math inline">\(K\)</span> is not too large, use OVO.</p></li>
</ul>
</section>
<section id="svm-vs-logistic-regression." class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="svm-vs-logistic-regression."><span class="header-section-number">9</span> SVM vs logistic regression.</h2>
<ul>
<li><p>With <span class="math inline">\(f(X) = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p\)</span>, the support vector classifier optimization can be recast as <span class="math display">\[
\min\limits_{\beta_0,\beta_1,\ldots,\beta_p} \left\{ \sum_{i=1}^n \max[0, 1 - y_i f(x_i)] + \lambda \sum_{j=1}^p \beta_j^2 \right\}.
\]</span> <strong>Hinge loss</strong> + <strong>ridge penalty</strong></p></li>
<li><p>The hinge loss is very similar to the negative log-likelihood of the logistic regression.</p></li>
</ul>
<p align="center">
<embed src="ISL_fig_9_12.pdf" width="400" height="400">
</p>
<ul>
<li><p>Which one to use?</p>
<ul>
<li><p>When classes are (nearly) separable, SVM does better than LR. So does LDA.</p></li>
<li><p>When not, LR (with ridge penalty) and SVM very similar.</p></li>
<li><p>If you wish to estimate probabilities, LR is the choice.</p></li>
<li><p>For nonlinear boundaries, kernel SVMs are popular. Can use kernels with LR and LDA as well, but computations are more expensive.</p></li>
</ul></li>
</ul>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>