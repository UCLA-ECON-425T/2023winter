<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Dr.&nbsp;Hua Zhou @ UCLA">
<meta name="dcterms.date" content="2023-01-22">

<title>Resampling Methods (ISL 5)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="resampling_files/libs/clipboard/clipboard.min.js"></script>
<script src="resampling_files/libs/quarto-html/quarto.js"></script>
<script src="resampling_files/libs/quarto-html/popper.min.js"></script>
<script src="resampling_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="resampling_files/libs/quarto-html/anchor.min.js"></script>
<link href="resampling_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="resampling_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="resampling_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="resampling_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="resampling_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#overview" id="toc-overview" class="nav-link active" data-scroll-target="#overview"><span class="toc-section-number">1</span>  Overview</a></li>
  <li><a href="#training-error-vs-test-error" id="toc-training-error-vs-test-error" class="nav-link" data-scroll-target="#training-error-vs-test-error"><span class="toc-section-number">2</span>  Training error vs test error</a></li>
  <li><a href="#validation-set-approach" id="toc-validation-set-approach" class="nav-link" data-scroll-target="#validation-set-approach"><span class="toc-section-number">3</span>  Validation-set approach</a></li>
  <li><a href="#k-fold-cross-validation" id="toc-k-fold-cross-validation" class="nav-link" data-scroll-target="#k-fold-cross-validation"><span class="toc-section-number">4</span>  <span class="math inline">\(K\)</span>-fold cross-validation</a>
  <ul>
  <li><a href="#loocv" id="toc-loocv" class="nav-link" data-scroll-target="#loocv"><span class="toc-section-number">4.1</span>  LOOCV</a></li>
  </ul></li>
  <li><a href="#bias-variance-tradeoff-for-cross-validation" id="toc-bias-variance-tradeoff-for-cross-validation" class="nav-link" data-scroll-target="#bias-variance-tradeoff-for-cross-validation"><span class="toc-section-number">5</span>  Bias-variance tradeoff for cross-validation</a></li>
  <li><a href="#cross-validation-for-classification-problems" id="toc-cross-validation-for-classification-problems" class="nav-link" data-scroll-target="#cross-validation-for-classification-problems"><span class="toc-section-number">6</span>  Cross-validation for classification problems</a></li>
  <li><a href="#bootstrap" id="toc-bootstrap" class="nav-link" data-scroll-target="#bootstrap"><span class="toc-section-number">7</span>  Bootstrap</a></li>
  <li><a href="#bootstrap-in-general" id="toc-bootstrap-in-general" class="nav-link" data-scroll-target="#bootstrap-in-general"><span class="toc-section-number">8</span>  Bootstrap in general</a></li>
  <li><a href="#pre-validation" id="toc-pre-validation" class="nav-link" data-scroll-target="#pre-validation"><span class="toc-section-number">9</span>  Pre-validation</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Resampling Methods (ISL 5)</h1>
<p class="subtitle lead">Econ 425T</p>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Dr.&nbsp;Hua Zhou @ UCLA </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">January 22, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Credit: This note heavily uses material from the books <a href="https://www.statlearning.com/"><em>An Introduction to Statistical Learning: with Applications in R</em></a> (ISL2) and <a href="https://hastie.su.domains/ElemStatLearn/"><em>Elements of Statistical Learning: Data Mining, Inference, and Prediction</em></a> (ESL2).</p>
<p>Display system information for reproducibility.</p>
<div class="panel-tabset">
<ul class="nav nav-tabs" role="tablist"><li class="nav-item" role="presentation"><a class="nav-link active" id="tabset-1-1-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-1" role="tab" aria-controls="tabset-1-1" aria-selected="true" href="">Python</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-2-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-2" role="tab" aria-controls="tabset-1-2" aria-selected="false" href="">R</a></li><li class="nav-item" role="presentation"><a class="nav-link" id="tabset-1-3-tab" data-bs-toggle="tab" data-bs-target="#tabset-1-3" role="tab" aria-controls="tabset-1-3" aria-selected="false" href="">Julia</a></li></ul>
<div class="tab-content">
<div id="tabset-1-1" class="tab-pane active" role="tabpanel" aria-labelledby="tabset-1-1-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> IPython</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(IPython.sys_info())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>{'commit_hash': 'add5877a4',
 'commit_source': 'installation',
 'default_encoding': 'utf-8',
 'ipython_path': '/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython',
 'ipython_version': '8.8.0',
 'os_name': 'posix',
 'platform': 'macOS-10.16-x86_64-i386-64bit',
 'sys_executable': '/Library/Frameworks/Python.framework/Versions/3.10/bin/python3',
 'sys_platform': 'darwin',
 'sys_version': '3.10.9 (v3.10.9:1dd9be6584, Dec  6 2022, 14:37:36) [Clang '
                '13.0.0 (clang-1300.0.29.30)]'}</code></pre>
</div>
</div>
</div>
<div id="tabset-1-2" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-2-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R version 4.2.2 (2022-10-31)
Platform: x86_64-apple-darwin17.0 (64-bit)
Running under: macOS Big Sur ... 10.16

Matrix products: default
BLAS:   /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRblas.0.dylib
LAPACK: /Library/Frameworks/R.framework/Versions/4.2/Resources/lib/libRlapack.dylib

locale:
[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

loaded via a namespace (and not attached):
 [1] Rcpp_1.0.9        here_1.0.1        lattice_0.20-45   png_0.1-8        
 [5] rprojroot_2.0.3   digest_0.6.30     grid_4.2.2        lifecycle_1.0.3  
 [9] jsonlite_1.8.4    magrittr_2.0.3    evaluate_0.18     rlang_1.0.6      
[13] stringi_1.7.8     cli_3.4.1         rstudioapi_0.14   Matrix_1.5-1     
[17] reticulate_1.26   vctrs_0.5.1       rmarkdown_2.18    tools_4.2.2      
[21] stringr_1.5.0     glue_1.6.2        htmlwidgets_1.6.0 xfun_0.35        
[25] yaml_2.3.6        fastmap_1.1.0     compiler_4.2.2    htmltools_0.5.4  
[29] knitr_1.41       </code></pre>
</div>
</div>
</div>
<div id="tabset-1-3" class="tab-pane" role="tabpanel" aria-labelledby="tabset-1-3-tab">
<div class="cell" data-layout-align="center">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">InteractiveUtils</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="fu">versioninfo</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
</div>
<section id="overview" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="overview"><span class="header-section-number">1</span> Overview</h2>
<ul>
<li><p>In the section we discuss two resampling methods: <strong>cross-validation</strong> and the <strong>bootstrap</strong>.</p></li>
<li><p>These methods refit a model of interest to samples formed from the training set, in order to obtain additional information about the fitted model.</p></li>
<li><p>For example, they provide estimates of test-set prediction error, and the standard deviation and bias of our parameter estimates.</p></li>
</ul>
</section>
<section id="training-error-vs-test-error" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="training-error-vs-test-error"><span class="header-section-number">2</span> Training error vs test error</h2>
<ul>
<li><p><strong>Test error</strong> is the average error that results from using a learning method to predict the response on a new observation, one that was not used in training the method.</p></li>
<li><p><strong>Training error</strong> can be easily calculated by applying the statistical learning method to the observations used in its training.</p></li>
<li><p>But the training error rate often is quite different from the test error rate, and in particular the former can dramatically underestimate the latter.</p></li>
<li><p>HW2 Bonus question rigorously justifies that the training error is an under-estimate of the test error.</p></li>
</ul>
<p><img src="training_vs_test.png" class="img-fluid"></p>
<ul>
<li><p>Best solution: a large designated test set. Often not available.</p></li>
<li><p>Some methods make a mathematical adjustment to the training error rate in order to estimate the test error rate. These include the <span class="math inline">\(C_p\)</span> statistic, AIC and BIC, which are discussed in Chapter 6.</p></li>
<li><p>Here we instead consider a class of methods that estimate the test error by <strong>holding out</strong> a subset of the training observations from the fitting process, and then applying the learning method to those held out observations.</p></li>
</ul>
</section>
<section id="validation-set-approach" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="validation-set-approach"><span class="header-section-number">3</span> Validation-set approach</h2>
<ul>
<li><p>Here we randomly divide the available set of samples into two parts: a <strong>training set</strong> and a <strong>validation</strong> or <strong>hold-out set</strong>.</p></li>
<li><p>The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.</p></li>
<li><p>The resulting validation-set error provides an estimate of the test error. This is typically assessed using MSE in the case of a quantitative response and misclassification rate in the case of a qualitative (discrete) response.</p></li>
</ul>
<div id="fig-validation-set" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_5_1.pdf" class="img-fluid" width="600">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: A random splitting into two halves: left part is training set, right part is validation set.</figcaption><p></p>
</figure>
</div>
<div id="fig-validation-auto" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_5_2.pdf" width="600" height="350">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;2: The validation set approach was used on the <code>Auto</code> data set in order to estimate the test error that results from predicting mpg using polynomial functions of <code>horsepower</code>. Left: Validation error estimates for a single split into training and validation data sets. Right: The validation method was repeated ten times, each time using a different random split of the observations into a training set and a validation set. This illustrates the variability in the estimated test MSE that results from this approach.</figcaption><p></p>
</figure>
</div>
<ul>
<li><p>Drawbacks of validation set approach</p>
<ul>
<li><p>The validation estimate of the test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.</p></li>
<li><p>In the validation approach, only a subset of the observations - those that are included in the training set rather than in the validation set - are used to fit the model.</p></li>
<li><p>This suggests that the validation set error may tend to <em>overestimate</em> the test error for the model fit on the entire data set. (Why?)</p></li>
</ul></li>
</ul>
</section>
<section id="k-fold-cross-validation" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="k-fold-cross-validation"><span class="header-section-number">4</span> <span class="math inline">\(K\)</span>-fold cross-validation</h2>
<ul>
<li><p><strong>Widely used approach</strong> for estimating test error.</p></li>
<li><p>Estimates can be used to select best model, and to give an idea of the test error of the final chosen model.</p></li>
<li><p>Idea is to randomly divide the data into <span class="math inline">\(K\)</span> equal-sized parts. We leave out part <span class="math inline">\(k\)</span>, fit the model to the other <span class="math inline">\(K-1\)</span> parts (combined), and then obtain predictions for the left-out <span class="math inline">\(k\)</span>th part.</p></li>
<li><p>This is done in turn for each part <span class="math inline">\(k = 1, 2, \ldots, K\)</span>, and then the results are combined.</p></li>
</ul>
<div id="fig-10-fold-cv-auto" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_5_5.pdf" width="600" height="350">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3: A schematic display of 5-fold CV. A set of <span class="math inline">\(n\)</span> observations is randomly split into five non-overlapping groups. Each of these fifths acts as a validation set (shown in beige), and the remainder as a training set (shown in blue). The test error is estimated by averaging the five resulting MSE estimates.</figcaption><p></p>
</figure>
</div>
<ul>
<li><p>Let the <span class="math inline">\(K\)</span> parts be <span class="math inline">\(C_1, C_2, \ldots, C_K\)</span>, where <span class="math inline">\(C_k\)</span> denotes the indices of the observations in part <span class="math inline">\(k\)</span>. There are <span class="math inline">\(n_k\)</span> observations in part <span class="math inline">\(k\)</span>. If <span class="math inline">\(N\)</span> is a multiple of K, then <span class="math inline">\(n_k = n / K\)</span>.</p></li>
<li><p>Compute <span class="math display">\[
\text{CV}_{(K)} = \sum_{k=1}^K \frac{n_k}{n} \text{MSE}_k,
\]</span> where <span class="math display">\[
\text{MSE}_k = \frac{1}{n_k} \sum_{i \in C_k} (y_i - \hat y_i)^2,
\]</span> and <span class="math inline">\(\hat y_i\)</span> is the fit for observation <span class="math inline">\(i\)</span>, obtained from the data with part <span class="math inline">\(k\)</span> removed.</p></li>
</ul>
<section id="loocv" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="loocv"><span class="header-section-number">4.1</span> LOOCV</h3>
<ul>
<li><p>The special case <span class="math inline">\(K=n\)</span> yields <span class="math inline">\(n\)</span>-fold or <strong>leave-one out cross-validation (LOOCV)</strong>.</p></li>
<li><p>With least-squares linear or polynomial regression, an amazing shortcut makes the cost of LOOCV the same as that of a single model fit! <span class="math display">\[
\text{CV}_{(n)} = \frac{1}{n} \sum_{i=1}^n \left( \frac{y_i - \hat y_i}{1 - h_i} \right)^2,
\]</span> where <span class="math inline">\(\hat y_i\)</span> is the <span class="math inline">\(i\)</span>th fitted value from the original least squares fit, and <span class="math inline">\(h_i\)</span> is the leverage (diagonal of the “hat” matrix). This is like the ordinary MSE, except the <span class="math inline">\(i\)</span>th residual is divided by <span class="math inline">\(1 - h_i\)</span>.</p></li>
<li><p>LOOCV sometimes useful, but typically doesn’t shake up the data enough. The estimates from each fold are highly correlated and hence their average can have high variance.</p></li>
<li><p>A better choice is <span class="math inline">\(K = 5\)</span> or 10.</p></li>
</ul>
<div id="fig-10-fold-cv-auto" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_5_4.pdf" width="600" height="350">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;4: Cross-validation was used on the <code>Auto</code> data set in order to estimate the test error that results from predicting <code>mpg</code> using polynomial functions of <code>horsepower</code>. Left: The LOOCV error curve. Right: 10-fold CV was run nine separate times, each with a different random split of the data into ten parts. The figure shows the nine slightly different CV error curves.</figcaption><p></p>
</figure>
</div>
<div id="fig-true-vs-estimated-test-MSE" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_5_6.pdf" width="600" height="350">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;5: Blue: true test MSE. Black dashed line: LOOCV estimate of test MSE. Orange: 10-fold CV estimate of test MSE. The crosses indicate the minimum of each of the MSE curves. <a href="https://ucla-econ-425t.github.io/2023winter/slides/02-statlearn/statlearn.html#fig-tradeoff-truth">Left data</a>. <a href="https://ucla-econ-425t.github.io/2023winter/slides/02-statlearn/statlearn.html#fig-tradeoff-smooth-truth">Middle data</a>. <a href="https://ucla-econ-425t.github.io/2023winter/slides/02-statlearn/statlearn.html#fig-tradeoff-wiggly-truth">Right data</a>.</figcaption><p></p>
</figure>
</div>
</section>
</section>
<section id="bias-variance-tradeoff-for-cross-validation" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="bias-variance-tradeoff-for-cross-validation"><span class="header-section-number">5</span> Bias-variance tradeoff for cross-validation</h2>
<ul>
<li><p>Since each training set is only <span class="math inline">\((K - 1) / K\)</span> as big as the original training set, the estimates of prediction error will typically be biased upward. (Why?)</p></li>
<li><p>This bias is minimized when <span class="math inline">\(K = n\)</span> (LOOCV), but this estimate has high variance, as noted earlier.</p></li>
<li><p><span class="math inline">\(K = 5\)</span> or <span class="math inline">\(10\)</span> provides a good compromise for this bias-variance trade-off.</p></li>
</ul>
</section>
<section id="cross-validation-for-classification-problems" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="cross-validation-for-classification-problems"><span class="header-section-number">6</span> Cross-validation for classification problems</h2>
<ul>
<li><p>We divide the data into <span class="math inline">\(K\)</span> roughly equal-sized parts <span class="math inline">\(C_1, C_2, \ldots, C_K\)</span>. <span class="math inline">\(C_k\)</span> denotes the indices of the observations in part <span class="math inline">\(k\)</span>. There are <span class="math inline">\(n_k\)</span> observations in part <span class="math inline">\(k\)</span>: if <span class="math inline">\(n\)</span> is a multiple of <span class="math inline">\(K\)</span>, then <span class="math inline">\(n_k = n / K\)</span>.</p></li>
<li><p>Compute <span class="math display">\[
\text{CV}_k = \sum_{k=1}^K \frac{n_k}{n} \text{Err}_k,
\]</span> where <span class="math display">\[
\text{Err}_k = \frac{1}{n_k} \sum_{i \in C_k} I(y_i \ne \hat y_i).
\]</span></p></li>
<li><p>The estimated standard deviation of <span class="math inline">\(\text{CV}_k\)</span> is <span class="math display">\[
\hat{\text{SE}}(\text{CV}_k) = \sqrt{\frac{1}{K} \sum_{k=1}^K \frac{(\text{Err}_k - \bar{\text{Err}_k})^2}{K-1}}.
\]</span> This is a useful estimate, but strictly speaking, not quite valid.</p></li>
</ul>
</section>
<section id="bootstrap" class="level2" data-number="7">
<h2 data-number="7" class="anchored" data-anchor-id="bootstrap"><span class="header-section-number">7</span> Bootstrap</h2>
<ul>
<li><p>The bootstrap is a flexible and powerful statistical tool that can be used to quantify the uncertainty associated with a given estimator or learning method.</p></li>
<li><p>For example, it can provide an estimate of the standard error of a coefficient, or a confidence interval for that coefficient.</p></li>
<li><p>A simple example.</p>
<ul>
<li><p>Suppose that we wish to invest a fixed sum of money in two financial assets that yield returns of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively, where <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random quantities.</p></li>
<li><p>We will invest a fraction <span class="math inline">\(\alpha\)</span> of our money in <span class="math inline">\(X\)</span>, and will invest the remaining <span class="math inline">\(1 - \alpha\)</span> in <span class="math inline">\(Y\)</span>.</p></li>
<li><p>We wish to choose <span class="math inline">\(\alpha\)</span> to minimize the total risk, or variance, of our investment. In other words, we want to minimize <span class="math inline">\(\operatorname{Var}(\alpha X + (1 - \alpha) Y)\)</span>.</p></li>
<li><p>One can show that the value that minimizes the risk is given by <span class="math display">\[
\alpha = \frac{\sigma_{Y}^2 - \sigma_{XY}}{\sigma_X^2 + \sigma_Y^2 - 2 \sigma_{XY}},
\]</span> where <span class="math inline">\(\sigma_X^2 = \operatorname{Var}(X)\)</span>, <span class="math inline">\(\sigma_Y^2 = \operatorname{Var}(Y)\)</span>, and <span class="math inline">\(\sigma_{XY} = \operatorname{Cov}(X, Y)\)</span>.</p></li>
<li><p>But the values of <span class="math inline">\(\sigma_X^2\)</span>, <span class="math inline">\(\sigma_Y^2\)</span>, and <span class="math inline">\(\sigma_{XY}\)</span> are unknown.</p></li>
<li><p>We can compute estimates for these quantities <span class="math inline">\(\hat{\sigma}_X^2\)</span>, <span class="math inline">\(\hat{\sigma}_Y^2\)</span>, and <span class="math inline">\(\hat{\sigma}_{XY}\)</span>, using a data set that contains measurements for <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>.</p></li>
<li><p>We can then estimate the value of <span class="math inline">\(\alpha\)</span> that minimizes the variance of our investment using <span class="math display">\[
\hat{\alpha} = \frac{\hat{\sigma}_{Y}^2 - \hat{\sigma}_{XY}}{\hat{\sigma}_X^2 + \hat{\sigma}_Y^2 - 2 \hat{\sigma}_{XY}}.
\]</span></p></li>
</ul></li>
</ul>
<div id="fig-unrealistic-toy" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_5_9.pdf" width="600" height="600">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;6: Each panel displays 100 simulated returns for investments <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. From left to right and top to bottom, the resulting estimates for <span class="math inline">\(\alpha\)</span> are 0.576, 0.532, 0.657, and 0.651.</figcaption><p></p>
</figure>
</div>
<ul>
<li><p>An unrealistic method:</p>
<ul>
<li><p>To estimate the standard deviation of <span class="math inline">\(\hat{\alpha}\)</span>, we repeated the process of simulating 100 paired observations of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, and estimating <span class="math inline">\(\alpha\)</span> 1,000 times.</p></li>
<li><p>We thereby obtained 1,000 estimates for <span class="math inline">\(\alpha\)</span>, which we can call <span class="math inline">\(\hat{\alpha}_1, \hat{\alpha}_2, \ldots, \hat{\alpha}_{1000}\)</span>.</p></li>
<li><p>For these simulations the parameters were set to <span class="math inline">\(\sigma_{X}^2 = 1\)</span>, <span class="math inline">\(\sigma_Y^2 = 1.25\)</span>, and <span class="math inline">\(\sigma_{XY} = 0.5\)</span>, and so we know that the true value of <span class="math inline">\(\alpha\)</span> is 0.6 (indicated by the red line).</p></li>
<li><p>The mean over 1,000 estimates for <span class="math inline">\(\alpha\)</span> is <span class="math display">\[
\bar{\alpha} = \frac{1}{1000} \sum_{r=1}^{1000} \hat{\alpha}_r = 0.5996,
\]</span> very close to <span class="math inline">\(\alpha = 0.6\)</span>, and the standard deviation of the estimates is <span class="math display">\[
\sqrt{\frac{1}{1000-1} \sum_{r=1}^{1000} (\hat{\alpha}_r - \bar{\alpha})^2} = 0.083.
\]</span> This gives us a very good idea of the accuracy of <span class="math inline">\(\hat{\alpha}\)</span>: <span class="math inline">\(\text{SE}(\hat{\alpha}) \approx 0.083\)</span>. So roughly speaking, for a random sample from the population, we would expect <span class="math inline">\(\hat{\alpha}\)</span> to differ from <span class="math inline">\(\alpha\)</span> by approximately 0.08, on average.</p></li>
</ul></li>
</ul>
<div id="fig-bootstrap-toy" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_5_11.pdf" width="600" height="600">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;7: A graphical illustration of the bootstrap approach on a small sample containing <span class="math inline">\(n = 3\)</span> observations. Each bootstrap data set contains <span class="math inline">\(n\)</span> observations, sampled with replacement from the original data set. Each bootstrap data set is used to obtain an estimate of <span class="math inline">\(\alpha\)</span>.</figcaption><p></p>
</figure>
</div>
<ul>
<li><p><strong>Bootstrap</strong> method.</p>
<ul>
<li><p>The procedure outlined above cannot be applied, because for real data we cannot generate new samples from the original population.</p></li>
<li><p>However, the bootstrap approach allows us to use a computer to mimic the process of obtaining new data sets, so that we can estimate the variability of our estimate without generating additional samples.</p></li>
<li><p>Rather than repeatedly obtaining independent data sets from the population, we instead obtain distinct data sets by repeatedly sampling observations from the original data set <strong>with replacement</strong>.</p></li>
<li><p>Each of these “bootstrap data sets” is created by sampling with replacement, and is the same size as our original dataset. As a result some observations may appear more than once in a given bootstrap data set and some not at all.</p></li>
<li><p>Denoting the first bootstrap data set by <span class="math inline">\(Z^{*1}\)</span>, we use <span class="math inline">\(Z^{*1}\)</span> to produce a new bootstrap estimate for <span class="math inline">\(\alpha\)</span>, which we call <span class="math inline">\(\hat{\alpha}^{*1}\)</span>.</p></li>
<li><p>This procedure is repeated <span class="math inline">\(B\)</span> times for some large value of <span class="math inline">\(B\)</span> (say 100 or 1000), in order to produce <span class="math inline">\(B\)</span> different bootstrap data sets <span class="math inline">\(Z^{*1}, Z^{*2}, \ldots, Z^{*B}\)</span>, and <span class="math inline">\(B\)</span> corresponding <span class="math inline">\(\alpha\)</span> estimates, <span class="math inline">\(\hat{\alpha}^{*1}, \hat{\alpha}^{*2}, \ldots, \hat{\alpha}^{*B}\)</span>.</p></li>
<li><p>We estimate the standard error of these bootstrap estimates using the formula <span class="math display">\[
\operatorname{SE}_B(\hat{\alpha}) = \sqrt{\frac{1}{B-1} \sum_{r=1}^B (\hat{\alpha}^{*r} - \bar{\hat{\alpha}}^{*})^2}.
\]</span></p></li>
<li><p>This serves as an estimate of the standard error of <span class="math inline">\(\hat{\alpha}\)</span> estimated from the original data set. For this example, <span class="math inline">\(\operatorname{SE}_B(\hat{\alpha}) = 0.087\)</span>.</p></li>
</ul></li>
</ul>
<div id="fig-unrealistic-vs-bootstrap-toy" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<embed src="ISL_fig_5_10.pdf" width="500" height="350">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8: Left: A histogram of the estimates of <span class="math inline">\(\alpha\)</span> obtained by generating 1,000 simulated data sets from the true population. Center: A histogram of the estimates of <span class="math inline">\(\alpha\)</span> obtained from 1,000 bootstrap samples from a single data set. Right: The estimates of <span class="math inline">\(\alpha\)</span> displayed in the left and center panels are shown as boxplots. In each panel, the pink line indicates the true value of <span class="math inline">\(\alpha\)</span>.</figcaption><p></p>
</figure>
</div>
</section>
<section id="bootstrap-in-general" class="level2" data-number="8">
<h2 data-number="8" class="anchored" data-anchor-id="bootstrap-in-general"><span class="header-section-number">8</span> Bootstrap in general</h2>
<div id="fig-bootstrap-general-pic" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p align="center">
<img src="bootstrap.png" class="img-fluid figure-img" width="500">
</p>
<p></p><figcaption class="figure-caption">Figure&nbsp;9: Bootstrap general scheme.</figcaption><p></p>
</figure>
</div>
<ul>
<li><p>In more complex data situations, figuring out the appropriate way to generate bootstrap samples can require some thought.</p>
<p>For example, if the data is a time series (e.g., stock prices), we can’t simply sample the observations with replacement (why not?). We can instead create blocks of consecutive observations, and sample those with replacements. Then we paste together sampled blocks to obtain a bootstrap dataset.</p></li>
<li><p>Other uses of the bootstrap.</p>
<ul>
<li><p>Primarily used to obtain standard errors of an estimate.</p></li>
<li><p>Also provides approximate confidence intervals for a population parameter. For example, looking at the histogram in the middle panel of histogram, the 5% and 95% quantiles of the 1000 values is (0.43, 0.72).</p></li>
<li><p>This is called a <strong>Bootstrap Percentile confidence interval</strong>. It is the simplest method (among many approaches) for obtaining a confidence interval from the bootstrap.</p></li>
</ul></li>
<li><p>Can the bootstrap estimate prediction error?</p>
<ul>
<li><p>In cross-validation, each of the <span class="math inline">\(K\)</span> validation folds is distinct from the other <span class="math inline">\(K-1\)</span> folds used for training: there is no overlap. This is crucial for its success.</p></li>
<li><p>To estimate prediction error using the bootstrap, we could think about using each bootstrap dataset as our training sample, and the original sample as our validation sample.</p></li>
<li><p>But each bootstrap sample has significant overlap with the original data. About two-thirds of the original data points appear in each bootstrap sample. (HW2)</p></li>
<li><p>This will cause the bootstrap to seriously underestimate the true prediction error.</p></li>
<li><p>The other way around - with original sample = training sample, bootstrap dataset = validation sample - is worse!</p></li>
<li><p>Can partly fix this problem by only using predictions for those observations that did not (by chance) occur in the current bootstrap sample.</p></li>
<li><p>But the method gets complicated, and in the end, cross-validation provides a simpler, more attractive approach for estimating prediction error.</p></li>
</ul></li>
</ul>
</section>
<section id="pre-validation" class="level2" data-number="9">
<h2 data-number="9" class="anchored" data-anchor-id="pre-validation"><span class="header-section-number">9</span> Pre-validation</h2>
<p>TODO</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>